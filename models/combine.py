import numpy as np
import soundfile as sf
import torch
import torchaudio

def combine_segments(mono_segments, target_segments, target_speaker, y, sr, output_path):
    print("üîú –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ü–µ–ª–µ–≤–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞...")

    all_chunks = []

    # --- 1. Mono —Å–µ–≥–º–µ–Ω—Ç—ã
    for start, end, speaker in mono_segments:
        if speaker == target_speaker:
            start_sample = int(start * sr)
            end_sample = int(end * sr)
            chunk = y[start_sample:end_sample]
            all_chunks.append((start, chunk))

    # --- 2. –†–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
    resampler = torchaudio.transforms.Resample(orig_freq=8000, new_freq=sr)

    for path, start, end in target_segments:
        chunk, chunk_sr = sf.read(path)

        if chunk_sr != sr:
            if chunk.ndim == 1:
                chunk = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0)
            else:
                chunk = torch.tensor(chunk.T, dtype=torch.float32)

            chunk = resampler(chunk).numpy()
            chunk = chunk.mean(axis=0) if chunk.ndim > 1 else chunk  # –º–æ–Ω–æ

        elif chunk.ndim == 2:
            chunk = chunk.mean(axis=1)

        all_chunks.append((start, chunk))

    # --- –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞
    all_chunks.sort(key=lambda x: x[0])

    # --- –°–∫–ª–µ–∏–≤–∞–µ–º
    combined_audio = np.concatenate([chunk for _, chunk in all_chunks])

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º
    sf.write(output_path, combined_audio, sr)
    print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π WAV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
