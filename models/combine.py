# models/combine.py

import numpy as np
import soundfile as sf
import torch
import torchaudio

def combine_segments(mono_segments, target_segments, target_speaker, y, sr, output_path):
    print("üîú –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ü–µ–ª–µ–≤–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞...")

    mono_target_segments = [
        (start, end) for start, end, speaker in mono_segments
        if speaker == target_speaker
    ]

    collected_chunks = []

    # --- 1. Mono —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ
    for start, end in mono_target_segments:
        start_sample = int(start * sr)
        end_sample = int(end * sr)
        chunk = y[start_sample:end_sample]
        collected_chunks.append(chunk)

    # --- 2. –†–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã (8000 ‚Üí 16000 –ì—Ü)
    resampler = torchaudio.transforms.Resample(orig_freq=8000, new_freq=sr)

    for path, start, end in target_segments:
        chunk, chunk_sr = sf.read(path)

        # –†–µ—Å–µ–º–ø–ª–∏—Ä—É–µ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if chunk_sr != sr:
            if chunk.ndim == 1:
                chunk = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0)
            else:
                chunk = torch.tensor(chunk.T, dtype=torch.float32)

            chunk = resampler(chunk).numpy()
            chunk = chunk.mean(axis=0) if chunk.ndim > 1 else chunk  # –º–æ–Ω–æ

        elif chunk.ndim == 2:
            chunk = chunk.mean(axis=1)

        collected_chunks.append(chunk)

    # --- –û–±—ä–µ–¥–∏–Ω—è–µ–º
    combined_audio = np.concatenate(collected_chunks)

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º
    sf.write(output_path, combined_audio, sr)
    print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π WAV —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
